{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HMM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY-8zxkwCHrG",
        "colab_type": "code",
        "outputId": "5414e30b-9fdc-4b59-a28a-1f9346a2ca8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from collections import defaultdict\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aumoACyC-Qr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dict():\n",
        "    dct = defaultdict()\n",
        "    dct['B-PER'] = defaultdict(int)\n",
        "    dct['I-PER'] = defaultdict(int)\n",
        "    dct['B-ORG'] = defaultdict(int)\n",
        "    dct['I-ORG'] = defaultdict(int)\n",
        "    dct['B-LOC'] = defaultdict(int)\n",
        "    dct['I-LOC'] = defaultdict(int)\n",
        "    dct['O'] = defaultdict(int)\n",
        "    return dct"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3N_mTCAGHje",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_noisy_label(label, prev_label, next_label):\n",
        "    if label in ['B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG', 'O']:\n",
        "        return label\n",
        "    else:\n",
        "        if (prev_label is None) or (next_label is None):\n",
        "            return 'O'\n",
        "        if label.endswith('MISC'):\n",
        "            return 'O'\n",
        "        if prev_label.startswith('B'):\n",
        "            if prev_label == 'B-PER':\n",
        "                return 'I-PER'\n",
        "            elif prev_label == 'B-LOC':\n",
        "                return 'I-LOC'\n",
        "            else:\n",
        "                return 'I-ORG'\n",
        "        elif next_label.startswith('I'):\n",
        "            if prev_label == 'O':\n",
        "                if next_label == 'I-PER':\n",
        "                    return 'B-PER'\n",
        "                elif next_label == 'I-LOC':\n",
        "                    return 'B-LOC'\n",
        "                else:\n",
        "                    return 'B-ORG'\n",
        "            elif prev_label.startswith('I'):\n",
        "                return prev_label\n",
        "        else:\n",
        "            return 'O'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqQnbCivC1TH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(data_path, data_format='1', test_size=0.15):\n",
        "    \"\"\"\n",
        "    Load data from file and split training set and test set\n",
        "    :param data_path: path to data folder\n",
        "    :param test_size: the ratio of test set to total dataset,\n",
        "        0 < test_size < 1, default = 0.15\n",
        "    :param data_format: form of a output data point:\n",
        "        '1': ('word', 'ner_label')\n",
        "        '2.1': ('word', 'pos-tagging label')\n",
        "        '2.2': ('word', 'chucking label', 'ner label')\n",
        "        '3': ('word', 'pos-tagging label', 'chucking label', 'ner label')\n",
        "        default = '1'\n",
        "    :return: training_set, test_set\n",
        "    \"\"\"\n",
        "    # Check input\n",
        "    if data_format not in {'1', '2.1', '2.2', '3'}:\n",
        "        raise Exception(\"{} not is a data_format. The value of data_format should in ('1', '2.1', '2.2', '3')\"\n",
        "                        .format(data_format))\n",
        "    if not os.path.exists(data_path):\n",
        "        raise Exception(\"{} does not exist\" .format(data_path))\n",
        "    if test_size <= 0 or test_size >= 1:\n",
        "        raise Exception(\"Test_size should be between 0 and 1. The value of test_size is: {}\" .format(test_size))\n",
        "\n",
        "    # load data\n",
        "    data = []\n",
        "    for file_name in os.listdir(data_path):\n",
        "        file_path = data_path + '/' + file_name\n",
        "        with open(file_path, encoding='utf-8') as f:\n",
        "            sentence = []\n",
        "            all_data = f.readlines()\n",
        "            for i in range(len(all_data)):\n",
        "                line = all_data[i]\n",
        "                label = line.split()\n",
        "                if len(label) > 4:\n",
        "                    # prev_label = all_data[i - 1].split()[3] if len(sentence) != 0 else None\n",
        "                    # next_label = all_data[i + 1].split()[3] if len(all_data[i + 1].split()) > 4 else None\n",
        "                    # label[3] = remove_noisy_label(label[3], prev_label, next_label)\n",
        "                    if label[3] not in ['B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG', 'O']:\n",
        "                        continue\n",
        "                    # all_data[i] = label[0] + '\t' + label[1] + '\t' + label[2] + '\t' + label[3] + '\t' + label[4]\n",
        "                    if data_format == '1':\n",
        "                        sentence.append((label[0], label[3]))\n",
        "                    elif data_format == '2.1':\n",
        "                        sentence.append((label[0], label[1], label[3]))\n",
        "                    elif data_format == '2.2':\n",
        "                        sentence.append((label[0], label[2], label[3]))\n",
        "                    else:\n",
        "                        sentence.append((label[0], label[1], label[2], label[3]))\n",
        "                else:\n",
        "                    data.append(sentence)\n",
        "                    sentence = []\n",
        "        f.close()\n",
        "\n",
        "    # split training set and test set\n",
        "    training_set, test_set = train_test_split(data, test_size=test_size, shuffle=True)\n",
        "    return training_set, test_set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znj9Q4-BQhT3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_transition_matrix_bigram(dataset, data_path):\n",
        "    transition_matrix = create_dict()\n",
        "    transition_matrix['start'] = defaultdict(int)\n",
        "    for sentence in dataset:\n",
        "        for i in range(len(sentence)):\n",
        "            ner = sentence[i][1]\n",
        "            if i == 0:\n",
        "                transition_matrix['start'][ner] += 1\n",
        "            elif i == len(sentence) - 1:\n",
        "                prev_ner = sentence[i - 1][1]\n",
        "                transition_matrix[prev_ner][ner] += 1\n",
        "                transition_matrix[ner]['stop'] += 1\n",
        "            else:\n",
        "                prev_ner = sentence[i - 1][1]\n",
        "                transition_matrix[prev_ner][ner] += 1\n",
        "    freq_matrix = defaultdict()\n",
        "    with open(data_path, 'w') as f:\n",
        "        for prev_ner in transition_matrix.keys():\n",
        "            curr_ner = transition_matrix[prev_ner]\n",
        "            total_label = sum(curr_ner.values())\n",
        "            freq_bigram = {}\n",
        "            for ner in curr_ner.keys():\n",
        "                freq_bigram[ner] = curr_ner[ner] / total_label\n",
        "                f.write((prev_ner + '<fff>' + ner + '<fff>' + str(freq_bigram[ner])))\n",
        "                f.write('\\n')\n",
        "            freq_matrix[prev_ner] = freq_bigram\n",
        "    f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZE-THxoQjHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_transition_matrix_trigram(dataset, data_path):\n",
        "    transition_matrix = defaultdict()\n",
        "    transition_matrix['start'] = create_dict()\n",
        "    transition_matrix['start']['start'] = defaultdict(int)\n",
        "    transition_matrix['B-PER'] = create_dict()\n",
        "    transition_matrix['I-PER'] = create_dict()\n",
        "    transition_matrix['B-ORG'] = create_dict()\n",
        "    transition_matrix['I-ORG'] = create_dict()\n",
        "    transition_matrix['B-LOC'] = create_dict()\n",
        "    transition_matrix['I-LOC'] = create_dict()\n",
        "    transition_matrix['O'] = create_dict()\n",
        "    for sentence in dataset:\n",
        "        for i in range(len(sentence)):\n",
        "            ner = sentence[i][1]\n",
        "            if i == 0:\n",
        "                transition_matrix['start']['start'][ner] += 1\n",
        "            elif i == 1:\n",
        "                prev1_ner = sentence[i - 1][1]\n",
        "                transition_matrix['start'][prev1_ner][ner] += 1\n",
        "            elif i == len(sentence) - 1:\n",
        "                prev2_ner = sentence[i - 2][1]\n",
        "                prev1_ner = sentence[i - 1][1]\n",
        "                transition_matrix[prev2_ner][prev1_ner][ner] += 1\n",
        "                transition_matrix[prev1_ner][ner]['stop'] += 1\n",
        "            else:\n",
        "                prev2_ner = sentence[i - 2][1]\n",
        "                prev1_ner = sentence[i - 1][1]\n",
        "                transition_matrix[prev2_ner][prev1_ner][ner] += 1\n",
        "    freq_matrix = defaultdict()\n",
        "    with open(data_path, 'w') as f:\n",
        "        for prev2_ner in transition_matrix.keys():\n",
        "            trigram = transition_matrix[prev2_ner]\n",
        "            freq_trigram = {}\n",
        "            for prev1_ner in trigram.keys():\n",
        "                bigram = trigram[prev1_ner]\n",
        "                freq_bigram = {}\n",
        "                total_label = sum(bigram.values())\n",
        "                for curr_ner in bigram.keys():\n",
        "                    freq_bigram[curr_ner] = bigram[curr_ner] / total_label\n",
        "                    f.write((prev2_ner + '<fff>' + prev1_ner + '<fff>' + curr_ner + '<fff>' + str(freq_bigram[curr_ner])))\n",
        "                    f.write('\\n')\n",
        "                freq_trigram[prev1_ner] = freq_bigram\n",
        "            freq_matrix[prev2_ner] = freq_trigram\n",
        "    f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fRfVt9dQo_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_emission_probability(dataset, data_path):\n",
        "    emission = create_dict()\n",
        "    vocab_size = defaultdict()\n",
        "    for sentence in dataset:\n",
        "        for i in range(len(sentence)):\n",
        "            word = sentence[i][0]\n",
        "            ner = sentence[i][1]\n",
        "            emission[ner][word] += 1\n",
        "    emission_probability = defaultdict()\n",
        "    with open(data_path, 'w', encoding='utf-8') as f:\n",
        "        for ner in emission.keys():\n",
        "            words = emission[ner]\n",
        "            total_label = sum(words.values())\n",
        "            vocab_size[ner] = len(words)\n",
        "            emission_bigram = {}\n",
        "            for word in words.keys():\n",
        "                emission_bigram[word] = (words[word] + 1) / (total_label + vocab_size[ner])\n",
        "                f.write((word + '<fff>' + ner + '<fff>' + str(emission_bigram[word])))\n",
        "                f.write('\\n')\n",
        "            emission_probability[ner] = emission_bigram\n",
        "    f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rr6tW6vQyHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training():\n",
        "    path = '/content/drive/My Drive/Colab Notebooks/NER/data/NER2016-training_data'\n",
        "    train, test = load_data(path)\n",
        "\n",
        "    emission_path = '/content/drive/My Drive/Colab Notebooks/NER/data/bigram_data'\n",
        "    create_emission_probability(train, emission_path + '/emission_probability_train.txt')\n",
        "    create_transition_matrix_bigram(train, emission_path + '/transition_matrix_train.txt')\n",
        "\n",
        "    emission_path = '/content/drive/My Drive/Colab Notebooks/NER/data/trigram_data'\n",
        "    create_emission_probability(train, emission_path + '/emission_probability_train.txt')\n",
        "    create_transition_matrix_trigram(train, emission_path + '/transition_matrix_train.txt')\n",
        "    return test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_najYjtpw28z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HMM_NER:\n",
        "    def __init__(self):\n",
        "        self.trigram = defaultdict()\n",
        "        self.trigram['start'] = create_dict()\n",
        "        self.trigram['start']['start'] = defaultdict(int)\n",
        "        self.trigram['B-PER'] = create_dict()\n",
        "        self.trigram['I-PER'] = create_dict()\n",
        "        self.trigram['B-ORG'] = create_dict()\n",
        "        self.trigram['I-ORG'] = create_dict()\n",
        "        self.trigram['B-LOC'] = create_dict()\n",
        "        self.trigram['I-LOC'] = create_dict()\n",
        "        self.trigram['O'] = create_dict()\n",
        "\n",
        "        self.bigram = create_dict()\n",
        "        self.bigram['start'] = defaultdict(int)\n",
        "\n",
        "        self.emission = create_dict()\n",
        "        self.states = {'B-PER': 0,\n",
        "                       'I-PER': 1,\n",
        "                       'B-LOC': 2,\n",
        "                       'I-LOC': 3,\n",
        "                       'B-ORG': 4,\n",
        "                       'I-ORG': 5,\n",
        "                       'O': 6}\n",
        "        self.eval_states1 = ['B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG']\n",
        "        self.eval_states = [0, 1, 2, 3, 4, 5]\n",
        "\n",
        "    def load_test_data(self, test_set):\n",
        "        x = []\n",
        "        y_word = []\n",
        "        for sentence in test_set:\n",
        "            if len(sentence) <= 0:\n",
        "                continue\n",
        "            xi = []\n",
        "            yi = []\n",
        "            for i in range(len(sentence)):\n",
        "                xi.append(sentence[i][0])\n",
        "                yi.append(self.states[sentence[i][1]])\n",
        "            x.append(xi)\n",
        "            y_word.extend(yi)\n",
        "        return x, y_word\n",
        "\n",
        "    def load_trigram(self, trigram_path):\n",
        "        for u in self.states.keys():\n",
        "            for v in self.states.keys():\n",
        "                for w in self.states.keys():\n",
        "                    self.trigram[u][v][w] = 0\n",
        "                    self.trigram['start']['start'][w] = 0\n",
        "                    self.trigram['start'][v][w] = 0\n",
        "        with open(trigram_path) as f:\n",
        "            for line in f.readlines():\n",
        "                line = line.strip()\n",
        "                y1, y2, y3, probability = line.split('<fff>')\n",
        "                self.trigram[y1][y2][y3] = probability\n",
        "\n",
        "    def load_bigram(self, bigram_path):\n",
        "        self.bigram['start']['start'] = 0\n",
        "        for u in self.states.keys():\n",
        "            for v in self.states.keys():\n",
        "                self.bigram[u][v] = 0\n",
        "            self.bigram['start'][u] = 0\n",
        "        with open(bigram_path) as f:\n",
        "            for line in f.readlines():\n",
        "                line = line.strip()\n",
        "                y1, y2, probability = line.split('<fff>')\n",
        "                self.bigram[y1][y2] = probability\n",
        "\n",
        "    def load_emission(self, emission_path):\n",
        "        with open(emission_path, encoding='utf-8') as f:\n",
        "            for line in f.readlines():\n",
        "                line = line.strip()\n",
        "                observation, state, probability = line.split('<fff>')\n",
        "                # self.emission[state] = defaultdict()\n",
        "                # self.emission[state][observation] = 0\n",
        "                self.emission[state][observation] = probability\n",
        "\n",
        "    def bigram_transition_probability(self, y1, y2):\n",
        "        return float(self.bigram[y1][y2])\n",
        "\n",
        "    def trigram_transition_probability(self, y1, y2, y3):\n",
        "        return float(self.trigram[y1][y2][y3])\n",
        "\n",
        "    def state_observation_likelihood(self, observation, state):\n",
        "        return float(self.emission[state][observation])\n",
        "\n",
        "    def rare_word_observation(self, state):\n",
        "        # print(1 / len(self.emission[state].keys()))\n",
        "        return 1 / len(self.emission[state].keys())\n",
        "\n",
        "    def bigram_decode(self, sentence):\n",
        "        viterbi = [defaultdict() for _ in range(len(sentence))]\n",
        "        back_point = [defaultdict() for _ in range(len(sentence) + 1)]\n",
        "        tag_seq = [0 for _ in range(len(sentence) + 1)]\n",
        "        word = sentence[0]\n",
        "        for state in self.states.keys():\n",
        "            # print(self.bigram_transition_probability('start', state))\n",
        "            # print(self.state_observation_likelihood(sentence[0], state))\n",
        "            # print('----------------')\n",
        "            state_vocab = [key for key in self.emission[state].keys()]\n",
        "            if word in state_vocab:\n",
        "                viterbi[0][state] = self.bigram_transition_probability('start', state) \\\n",
        "                                    * self.state_observation_likelihood(word, state)\n",
        "            else:\n",
        "                viterbi[0][state] = self.bigram_transition_probability('start', state) \\\n",
        "                                    * self.rare_word_observation(state)\n",
        "            # print(state + ': ' + str(self.bigram['start'][state]))\n",
        "            back_point[0][state] = 0\n",
        "            # tag_seq[0] = 0\n",
        "        for i in range(1, len(sentence)):\n",
        "            word = sentence[i]\n",
        "            for v in self.states.keys():\n",
        "                max_score = 0\n",
        "                tag = None\n",
        "                for u in self.states.keys():\n",
        "                    # print(self.emission[v].keys())\n",
        "                    if word in self.emission[v].keys():\n",
        "                        # print(1)\n",
        "                        score = viterbi[i - 1][u] \\\n",
        "                                * self.bigram_transition_probability(u, v) \\\n",
        "                                * self.state_observation_likelihood(word, v)\n",
        "                    else:\n",
        "                        score = viterbi[i - 1][u] \\\n",
        "                                * self.bigram_transition_probability(u, v) \\\n",
        "                                * self.rare_word_observation(v)\n",
        "                    if score > max_score:\n",
        "                        max_score = score\n",
        "                        tag = self.states[u]\n",
        "                viterbi[i][v] = max_score\n",
        "                back_point[i][v] = tag\n",
        "                # tag_seq[i] = tag\n",
        "\n",
        "        max_score = 0\n",
        "        tag = None\n",
        "        for u in self.states.keys():\n",
        "            score = viterbi[len(sentence) - 1][u] \\\n",
        "                    * self.bigram_transition_probability(u, 'stop')\n",
        "            if score > max_score:\n",
        "                max_score = score\n",
        "                tag = self.states[u]\n",
        "            if tag is None:\n",
        "                print(viterbi[len(sentence) - 1][u] \\\n",
        "                     * self.bigram_transition_probability(u, 'stop'))\n",
        "        best_prob = max_score\n",
        "        tag_seq[len(sentence)] = tag\n",
        "        back_point[len(sentence)]['stop'] = tag\n",
        "\n",
        "        for k in range(len(sentence) - 1, 0, -1):\n",
        "            # print(tag_seq[k + 1])\n",
        "            ner = (list(self.states.keys()))[list(self.states.values()).index(tag_seq[k + 1])]\n",
        "            tag_seq[k] = back_point[k][ner]\n",
        "        # print(tag_seq)\n",
        "\n",
        "        return best_prob, tag_seq[1:]\n",
        "\n",
        "    def trigram_decode(self, sentence):\n",
        "        viterbi = [defaultdict() for _ in range(len(sentence))]\n",
        "        back_point = [defaultdict() for _ in range(len(sentence) + 1)]\n",
        "        tag_seq = [0 for _ in range(len(sentence) + 1)]\n",
        "        word = sentence[0]\n",
        "        for state in self.states.keys():\n",
        "            state_vocab = [key for key in self.emission[state].keys()]\n",
        "            if word in state_vocab:\n",
        "                viterbi[0][state] = self.trigram_transition_probability('start', 'start', state) \\\n",
        "                                    * self.state_observation_likelihood(word, state)\n",
        "            else:\n",
        "                viterbi[0][state] = self.trigram_transition_probability('start', 'start', state) \\\n",
        "                                    * self.rare_word_observation(state)\n",
        "            # print(state + ': ' + str(self.bigram['start'][state]))\n",
        "            back_point[0][state] = [-1, -1]\n",
        "            # tag_seq[0] = 0\n",
        "        for i in range(1, len(sentence)):\n",
        "            word = sentence[i]\n",
        "            for w in self.states.keys():\n",
        "                max_score = 0\n",
        "                tag = None\n",
        "                for v in self.states.keys():\n",
        "                    for u in self.states.keys():\n",
        "                        # print(self.emission[v].keys())\n",
        "                        if word in self.emission[v].keys():\n",
        "                            # print(1)\n",
        "                            score = viterbi[i - 1][v] \\\n",
        "                                    * self.trigram_transition_probability(u, v, w) \\\n",
        "                                    * self.state_observation_likelihood(word, w)\n",
        "                        else:\n",
        "                            score = viterbi[i - 1][v] \\\n",
        "                                    * self.trigram_transition_probability(u, v, w) \\\n",
        "                                    * self.rare_word_observation(w)\n",
        "                        if score > max_score:\n",
        "                            max_score = score\n",
        "                            tag = [self.states[u], self.states[v]]\n",
        "                viterbi[i][w] = max_score\n",
        "                back_point[i][w] = tag\n",
        "                # tag_seq[i] = tag\n",
        "\n",
        "        max_score = 0\n",
        "        tag = None\n",
        "        for v in self.states.keys():\n",
        "            for u in self.states.keys():\n",
        "                score = viterbi[len(sentence) - 1][v] \\\n",
        "                        * self.trigram_transition_probability(u, v, 'stop')\n",
        "                if score > max_score:\n",
        "                    max_score = score\n",
        "                    tag = [self.states[u], self.states[v]]\n",
        "        best_prob = max_score\n",
        "        tag_seq[len(sentence)] = tag[1]\n",
        "        back_point[len(sentence)]['stop'] = tag\n",
        "\n",
        "        for k in range(len(sentence) - 1, -1, -1):\n",
        "            ner = (list(self.states.keys()))[list(self.states.values()).index(tag_seq[k + 1])]\n",
        "            tag_seq[k] = back_point[k][ner][1]\n",
        "\n",
        "        return best_prob, tag_seq[1:]\n",
        "\n",
        "    def evaluate(self, sentences, labels, method='bigram'):\n",
        "        assert method in ['bigram', 'trigram']\n",
        "        y_predict = []\n",
        "        for sentence in sentences:\n",
        "            if method == 'bigram':\n",
        "                prob, state_seq = self.bigram_decode(sentence)\n",
        "                y_predict.extend(state_seq)\n",
        "            if method == 'trigram':\n",
        "                prob, state_seq = self.trigram_decode(sentence)\n",
        "                y_predict.extend(state_seq)\n",
        "        acc = accuracy_score(labels, y_predict)\n",
        "        print('accuracy: ', acc)\n",
        "        prec, rec, f1, support = precision_recall_fscore_support(labels, y_predict, labels=self.eval_states)\n",
        "        all_prec = precision_score(labels, y_predict, labels=self.eval_states, average='micro')\n",
        "        all_rec = recall_score(labels, y_predict, labels=self.eval_states, average='micro')\n",
        "        all_f1 = f1_score(labels, y_predict, labels=self.eval_states, average='micro')\n",
        "        print('labels:    {}'.format(self.eval_states1))\n",
        "        print('precision: {}'.format([str(round(p * 100, 2)) + '%' for p in prec]))\n",
        "        print('recall:    {}'.format([str(round(r * 100, 2)) + '%' for r in rec]))\n",
        "        print('f1-score:  {}'.format([str(round(f * 100, 2)) + '%' for f in f1]))\n",
        "        print('support:   {}'.format(support))\n",
        "        print('average precision: ', all_prec)\n",
        "        print('average recall: ', all_rec)\n",
        "        print('average f1-score: ', all_f1)\n",
        "\n",
        "    def predict(self, sentence, method='bigram'):\n",
        "        assert method in ['bigram', 'trigram']\n",
        "        y_predict = []\n",
        "        if method == 'bigram':\n",
        "            prob, state_seq = self.bigram_decode(sentence)\n",
        "            y_predict.extend(state_seq)\n",
        "        if method == 'trigram':\n",
        "            prob, state_seq = self.trigram_decode(sentence)\n",
        "            y_predict.extend(state_seq)\n",
        "        return y_predict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf0ptOMOCded",
        "colab_type": "code",
        "outputId": "aaa0ed61-5061-431e-c0c7-f3ddc4b4bf20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    hmm = HMM_NER()\n",
        "    test = training()\n",
        "    x_test, y_test_word = hmm.load_test_data(test)\n",
        "    hmm.load_bigram('/content/drive/My Drive/Colab Notebooks/NER/data/bigram_data/transition_matrix_train.txt')\n",
        "    hmm.load_emission('/content/drive/My Drive/Colab Notebooks/NER/data/bigram_data/emission_probability_train.txt')\n",
        "    hmm.evaluate(x_test, y_test_word, method='bigram')\n",
        "    hmm.load_trigram('/content/drive/My Drive/Colab Notebooks/NER/data/trigram_data/transition_matrix_train.txt')\n",
        "    hmm.evaluate(x_test, y_test_word, method='trigram')\n",
        "    "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy:  0.5788295548153719\n",
            "labels:    ['B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG']\n",
            "precision: ['15.7%', '6.11%', '73.86%', '53.97%', '3.41%', '2.42%']\n",
            "recall:    ['47.52%', '77.45%', '20.14%', '25.25%', '28.02%', '88.25%']\n",
            "f1-score:  ['23.61%', '11.33%', '31.64%', '34.4%', '6.08%', '4.72%']\n",
            "support:   [1111  541  884  404  182  349]\n",
            "average precision:  0.06381266596926048\n",
            "average recall:  0.45692883895131087\n",
            "average f1-score:  0.11198587819947044\n",
            "accuracy:  0.9575767343576519\n",
            "labels:    ['B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG']\n",
            "precision: ['71.73%', '80.77%', '42.58%', '55.5%', '24.37%', '70.66%']\n",
            "recall:    ['73.99%', '89.28%', '83.71%', '77.48%', '74.18%', '64.18%']\n",
            "f1-score:  ['72.84%', '84.81%', '56.45%', '64.67%', '36.68%', '67.27%']\n",
            "support:   [1111  541  884  404  182  349]\n",
            "average precision:  0.5525727069351231\n",
            "average recall:  0.7827715355805244\n",
            "average f1-score:  0.6478302336671435\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}