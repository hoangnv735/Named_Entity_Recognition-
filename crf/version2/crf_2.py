# -*- coding: utf-8 -*-
"""crf-2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17LVAgNRAskYe_5DHmoANhL2OyDyVeTNk
"""

from google.colab import drive
drive.mount('/content/drive')

# gensim modules
from gensim import utils
from gensim.models.doc2vec import LabeledSentence
from gensim.models import Word2Vec

#tune hyperparam
from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform

import numpy as np

class Config():
  # directory of raw VLSP data
  rawdata_path = '/content/drive/My Drive/NLP/NER/dataVLSP/'
  # directory of VLSP data  has been removed the xml tags
  data_path = '/content/drive/My Drive/NLP/NER/data/'
  # word2vec trained file
  embedd_pathfile = '/content/drive/My Drive/NLP/NER/word2vec.ipynb'
  #labels
  labels = ['B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'O']

config = Config()

#clean data

import codecs
import os

config = Config()

def remove_xml_tags(filename):
  ''' 
  Remove xml tag in file in data folder(raw data)
  Args:
    filename: The name of the data file in dataVLSP folder
  Return:
    File of the same name has removed xml tags in data folder
  Example:
    <editor>Vietlex team, 8-2016</editor>
    -DOCSTART-
    <s>				
    Đó	P	B-NP	O	O
    là	V	B-VP	O	O
    con	Nc	B-NP	O	O
  :converted into:
    Đó	P	B-NP	O	O
    là	V	B-VP	O	O
    con	Nc	B-NP	O	O

    saved in dataVLSP folder(processed data)
  '''
  f1 = open(config.rawdata_path + filename, 'r',encoding='utf-8')
  f2 = open(config.data_path + filename, 'w+',encoding='utf-8')
  for line in f1:
    line.strip()
    if(('<title>' in line) or line.startswith('<e') or line.startswith('-D') or line.startswith('<s>')):
      pass
    elif(line.startswith('</')):
      f2.write(line.replace(line,'\n'))
    else:
      f2.write(line)
  f1.close()
  f2.close()

def clean_data(path):
  ''' 
  Remove xml tags of all files in the dataVLSP folder
  Processed data saved in data
  '''
  list_files = os.listdir(path)
  for file in list_files:
    remove_xml_tags(file)

  clean_data(config.rawdata_path)

from sklearn.model_selection import train_test_split
import os
import codecs
config = Config()
def prepare_data(path, scale,index_attri):
  ''' Create training data and testing data
      Format of data: CoNLL

      Args:
        path: path of data folder
        scale: test size
        index_attri: Represents the number of attributes and the associated attribute type
          index_attri == 1 : The number of attributes = 1 - only ner label. ex: [('Huế', 'B_LOC'), ('là', 'O'), ('thành_phố', 'O'), ('đẹp', 'O')]
          index_attri == 2.1 : The number of attributes = 2(pos-tagging label, ner label). ex: [('Đó', 'P', 'O'), ('là', 'V',  'O'), ('con', 'Nc', 'O'), ('đường', 'N', , 'O')]
          index_attri = 2.2 : The number of attributes = 2(chunking label, ner label). ex: [('Đó', 'B-NP', 'O'), ('là', 'B-VP', 'O'), ('con', 'B-NP', 'O'), ('đường', 'B-NP', 'O')]
          index_attri = 3 : The number of attributes = 3(pos-tagging label,chunking, ner label). ex: [('Đó', 'P', 'B-NP', 'O'), ('là', 'V', 'B-VP', 'O'), ('con', 'Nc', 'B-NP', 'O'), ('đường', 'N', 'B-NP', 'O')]
          if index_attri not in {1,2.1,2,2,3} index_attri = 2.1
      Return:
        train_sents, test_sents
      
      Example of format data:
      [[('Đó', 'P', 'B-NP', 'O'), ('là', 'V', 'B-VP', 'O'), ('con', 'Nc', 'B-NP', 'O'), ('đường', 'N', 'B-NP', 'O')],
      [('Đó', 'P', 'B-NP', 'O'), ('là', 'V', 'B-VP', 'O'), ('con', 'Nc', 'B-NP', 'O'), ('đường', 'N', 'B-NP', 'O')],
      ...
      ]

  '''

  # check index_attri
  if index_attri not in {1,2.1,2,2,3} :
    index_attri = 2.1
  # split data by file
  list_files = os.listdir(path)
  # train_files, test_files = train_test_split(list_files,test_size=scale,random_state=42)
  all_data = []

  ''' Convert data format to CoNll '''
  #training data
  for file in list_files:
    with codecs.open(path + file,'r',encoding='utf8') as f:
      sentence = []
      remove = False;
      for line in f:
        line = line.split()
        if len(line) > 3 :
          if(line[3] not in config.labels):
            remove = True
          else:
            if index_attri == 1:
                sentence.append((line[0],line[3]))
            elif index_attri == 2.2:
              sentence.append((line[0],line[2],line[3]))
            elif index_attri == 3:
              sentence.append((line[0],line[1],line[2],line[3]))
            else:
              sentence.append((line[0],line[1],line[3]))
        else:
          if len(sentence) > 0:
            if remove == False:                            
              all_data.append(sentence)
            else:
              remove = False
            sentence = []
    f.close()
  
  train_sent_data, test_sent_data = train_test_split(all_data,test_size=scale,random_state=42)
  return  train_sent_data,test_sent_data

train_sents,test_sents = prepare_data(config.data_path,0.15,2.1)
print(len(train_sents))
print(len(test_sents))

pip install sklearn_crfsuite

pip install eli5

cd '/content/drive/My Drive/NLP/NER/'

re_adm_div      = ['ấp', 'buôn', 'bản', 'huyện', 'làng', 'miền', 'nước', 
                   'phường', 'quận', 'tỉnh', 'thành_phố', 'thị_trấn', 'thị_xã', 
                   'thôn', 'TT', 'TP', 'TX', 'TT.', 'TP.', 'TX.', 'xứ', 'xã', 
                   'xóm']
re_org          = ['báo', 'bệnh_viện', 'bệnh_xá', 'công_ty', 'công_ti', 'đài', 'đảng', 'đoàn', 'hội', 'hợp_tác_xã', 'khách_sạn', 'nhà_máy', 'nhà_xuất_bản', 'ngân_hàng', 'quỹ', 'tạp_chí', 'tập đoàn', 'thông_tấn_xã', 'tờ', 'trạm_xá', 'xí_nghiệp','ủy_ban']
re_school       = ['mẫu_giáo', 'tiểu_học', 'trung_học', 'trung_học_cơ_sở', 
                   'trung_học_phổ_thông', 'cao_đẳng', 'trung_cấp', 
                   'trung_cấp_nghề', 'đại_học']
re_street       = ['đại_lộ', 'đường', 'hẻm', 'ngách', 'ngõ', 'nhà', 'phố', 'quốc_lộ']
re_place        = ['ao', 'am', 'bến', 'bến_cảng', 'bến_phà','biển', 'cảng', 
                   'cầu', 'công_viên', 'chợ', 'chùa', 'dãy', 'đảo', 'đầm', 'đèo', 
                   'đền', 'đình', 'đồi', 'động', 'đồng_bằng', 'gềnh', 'gò', 'khu', 'hòn', 'hồ', 
                   'lăng', 'miếu', 'miền', 'nhà_ga', 'núi', 'phà', 'quần_đảo', 
                   'sân_bay', 'sông', 'suối', 'vùng']
re_office       = ['ban', 'bộ', 'chi_cục', 'cục', 'hạt', 'sở']
re_army         = ['binh_đoàn', 'đại_đội', 'đặc_khu', 'đơn_vị', 'lữ_đoàn', 'quân_đoàn', 'quân_đội', 'quân_khu','sư_đoàn', 'tiểu_đội', 'tiểu_đoàn', 'trung_đội']

def re_word(word):
    """
        Return a dict of (regexp Name, regexp Value) of a word
        :type word: string
        :param word: a word in sentence
    """

    check_code = False
    for char in word:
        if char.isdigit():
            check_code = True
            break

def re_word_org(word):
  return word.lower() in re_org

def re_word_name(word):
  return word[0].isupper()

def re_word_capital(word):
  return word.isupper()

def re_word_adm_div(word):
  return word.lower() in re_adm_div

def re_word_is_school(word):
  return word.lower() == 'trường'

def re_word_street(word):
  return word.lower() in re_street

def re_word_place(word):
  return word.lower() in re_place

def re_word_office(word):
  return word in re_office

def re_word_army(word):
  return word in re_army

def check_code(word):
  check_code = False
  for char in word:
      if char.isdigit():
          check_code = True
          break; 
  return check_code

def is_mix(word):
  return not(word.islower() and word.isupper())

import pandas as pd
import pycrfsuite
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support
from sklearn.preprocessing import LabelBinarizer
from itertools import chain
import re
def word2feature(sent, i):
  word = sent[i][0]
  
  # if '_' in word:
    # word.replace('_', ' ')
  #print(sent[i],'\n')
  #Tag = sent[i][2]

  features = ([
      'bias',
      'sentence_[-3:]='+word[-3:],
      'sentence_[-2:]='+word[-2:],
      'sentence_.lower=%s'%word.lower(),
      'sentence_.isupper=%s'%word.isupper(),
      'sentence_.istitle=%s' % word.istitle(),
      'sentence_.is_mix=%s' % is_mix(word),
      'sentence_.is_capital_period=%s' % (('.' in word) and word[0].isupper()),
      'sentence_.isdigit=%s' % word.isdigit(),
      'sentence_end_digit=%s' %word[-1].isdigit(),
      'sentence_.hashyphen=%s' % word.find('-'),
      'sentence_.is_code=%s' % check_code(word),
      'sentence_.num_syllabus=%s' %(word.count('_') + 1),
      'sentence_.is_name=%s' % word[0].isupper(),
      'sentence_.re_word_org=%s'%re_word_org(word),
      'sentence_.re_word_name=%s'%re_word_name(word),
      'sentence_.re_word_capital=%s'%re_word_capital(word),
      'sentence_.re_word_adm_div=%s'%re_word_adm_div(word),
      'sentence_.re_word_is_school=%s'%re_word_is_school(word),
      'sentence_.re_word_street=%s'%re_word_street(word),
      'sentence_.re_word_place=%s'%re_word_place(word),
      'sentence_.re_word_office=%s'%re_word_office(word),
      'sentence_.re_word_army=%s'%re_word_army(word),
       #'tag='+ Tag
  ])
  '''
  wordembdding=get_features(word)
  for iv,value in enumerate(wordembdding):
    features.extend(['v{}'.format(iv) +': '+ str(value)])
  '''
  if i > 0:
    word1 = sent[i - 1][0]
    if '_' in word1:
      word.replace('_', ' ')
    Tag1 = sent[i - 1][2]
    features.extend([
        '-1:sentence_.lower=' + word1.lower(),
        '-1:sentence_.istitle=%s' % word1.istitle(),
        '-1:sentence_.isupper=%s' % word1.isupper(),
        '-1:sentence_.isdigit=%s' % word1.isdigit(),
        #'-1:tag=' + Tag1,
        # '-1:tag[:2]=' + Tag1[:2],
    ])
  else:
    features.append('BOS')
  if i < len(sent)-1:
    word1 = sent[i+1][0]
    if '_' in word1:
      word.replace('_', ' ')
    #Tag1 = sent[i+1][2]
    # print(word1,'_',Tag1)

    features.extend([
        '+1:sentence_.lower=' + word1.lower(),
        '+1:sentence_.istitle=%s' % word1.istitle(),
        '+1:sentence_.isupper=%s' % word1.isupper(),
        '+1:sentence_.isdigit=%s' % word1.isdigit(),
        #'+1:tag=' + Tag1,
        # '+1:tag[:2]=' + Tag1[:2],
    ])
  else:
    features.append('EOS')

  return features



def sent2features(sent):
  return [word2feature(sent, i) for i in range(len(sent))]

def sent2labels(sent):
  return [label for token, postag, label in sent]

def sent2tokens(sent):
  return [token for token, postag, label in sent]

X_train = [sent2features(s) for s in train_sents]
y_train = [sent2labels(s) for s in train_sents]

X_train[0][0]

import nltk
import sklearn_crfsuite
import eli5
crf = sklearn_crfsuite.CRF(
  algorithm='lbfgs',
  c1=0.1,
  c2= 1e-17,
  max_iterations=50,
)

a = 1331
#crf.fit(X_train,y_train)
#crf.fit(X_train[:a],y_train[:a])
crf.fit(X_train[a+1:]+ X_train[:a], y_train[a+1:]+y_train[:a])
#crf.fit(X_train[a+1:], y_train[a+1:])

# Save Model Using joblib
import pandas
from sklearn import model_selection
from sklearn.linear_model import LogisticRegression
import joblib

# save the model to disk
filename = 'base_model2.sav'
joblib.dump(crf, filename)
  
# load the model from disk
#loaded_model = joblib.load(filename)
# result = loaded_model.score(X_test, Y_test)
#print(result)

eli5.show_weights(crf, top=30)

eli5.show_weights(crf, top=5, show=['transition_features'])

from itertools import chain

import nltk
import sklearn
import scipy.stats
from sklearn.metrics import make_scorer

import sklearn_crfsuite
from sklearn_crfsuite import scorers
from sklearn_crfsuite import metrics

labels = list(crf.classes_)
labels

labels = [
 'B-LOC',
 'I-LOC',
 'B-ORG',
 'I-ORG',
 'B-PER',
 'I-PER']

X_test = [sent2features(s) for s in test_sents]
y_test = [sent2labels(s) for s in test_sents]

y_pred = crf.predict(X_test)
metrics.flat_f1_score(y_test, y_pred,average='weighted', labels=labels)

sorted_labels = sorted(
    labels,
    key=lambda name: (name[1:], name[0])
)
print(metrics.flat_classification_report(
    y_test, y_pred, labels=sorted_labels, digits=3
))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# crf = sklearn_crfsuite.CRF(
#     algorithm='lbfgs',
#     max_iterations=100,
#     all_possible_transitions=True
# )
# params_space = {
#     'c1': scipy.stats.expon(scale=0.5),
#     'c2': scipy.stats.expon(scale=0.05),
# }
# 
# # use the same metric for evaluation
# f1_scorer = make_scorer(metrics.flat_f1_score,
#                         average='weighted', labels=labels)
# 
# # search
# rs = RandomizedSearchCV(crf, params_space,
#                         cv=3,
#                         verbose=1,
#                         n_jobs=-1,
#                         n_iter=50,
#                         scoring=f1_scorer)
# rs.fit(X_train[a+1:]+ X_train[:a], y_train[a+1:]+y_train[:a])

print('best params:', rs.best_params_)
print('best CV score:', rs.best_score_)
print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))

crf = rs.best_estimator_
y_pred = crf.predict(X_test)
print('f1_scorer:')
print(metrics.flat_f1_score(y_test, y_pred,average='weighted', labels=labels))
print('precision:')
print(metrics.flat_precision_score(y_test, y_pred,average='weighted', labels=labels))
print('recall:')
print(metrics.flat_recall_score(y_test, y_pred,average='weighted', labels=labels))
print('accuracy:')
print(metrics.flat_accuracy_score(y_test, y_pred))

filename = 'model2.sav'
joblib.dump(crf, filename)

# crf = rs.best_estimator_
# y_pred = crf.predict(X_test)
print(metrics.flat_classification_report(
    y_test, y_pred, labels=sorted_labels, digits=3
))